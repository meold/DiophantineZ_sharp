// Generated by Hybridizer version 1.0.0.0
 #include "cuda_runtime.h"                                                                                 
 #include "device_launch_parameters.h"                                                                     
                                                                                                             
 #if defined(__CUDACC__)                                                                                     
 	#ifndef hyb_device                                                                                       
 		#define hyb_inline __forceinline__                                                                   
 		                                                                                                     
 		#define hyb_constant __constant__                                                                    
 		#if defined(HYBRIDIZER_NO_HOST)                                                                      
 			#define hyb_host                                                                                 
 			#define	hyb_device  __device__                                                                   
 		#else                                                                                                
 			#define hyb_host __host__                                                                        
 			#define	hyb_device  __device__                                                                   
 		#endif                                                                                               
 	#endif                                                                                                   
 #else                                                                                                        
 	#ifndef hyb_device                                                                                       
 		#define hyb_inline inline                                                                            
 		#define hyb_device                                                                                   
 		#define hyb_constant                                                                                 
 	#endif                                                                                                   
 #endif                                                                                                       
                                                                                                              
 #pragma once                                                                                                 
 #if defined _WIN32 || defined _WIN64 || defined __CYGWIN__                                                   
   #define BUILDING_DLL                                                                                       
   #ifdef BUILDING_DLL                                                                                        
     #ifdef __GNUC__                                                                                          
       #define DLL_PUBLIC __attribute__ ((dllexport))                                                         
     #else                                                                                                    
       #define DLL_PUBLIC __declspec(dllexport) // Note: actually gcc seems to also supports this syntax.     
     #endif                                                                                                   
   #else                                                                                                      
     #ifdef __GNUC__                                                                                          
       #define DLL_PUBLIC __attribute__ ((dllimport))                                                         
     #else                                                                                                    
       #define DLL_PUBLIC __declspec(dllimport) // Note: actually gcc seems to also supports this syntax.     
     #endif                                                                                                   
   #endif                                                                                                     
   #define DLL_LOCAL                                                                                          
 #else                                                                                                        
   #if __GNUC__ >= 4                                                                                          
     #define DLL_PUBLIC __attribute__ ((visibility ("default")))                                            
     #define DLL_LOCAL  __attribute__ ((visibility ("hidden")))                                             
   #else                                                                                                      
     #define DLL_PUBLIC                                                                                       
     #define DLL_LOCAL                                                                                        
   #endif                                                                                                     
 #endif                                                                                                       


// hybridizer core types
#include <cstdint>
namespace hybridizer { struct hybridobject ; }
namespace hybridizer { struct runtime ; }

#pragma region defined enums and types
#ifndef __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__
#define __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__
namespace Hybridizer { namespace Runtime { namespace CUDAImports { 
enum struct ResidentArrayStatus
{
	NoAction = 0, 
	DeviceNeedsRefresh = 1, 
	HostNeedsRefresh = 2, 
} ;
} } } // Leaving namespace
#endif // __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__

// Intrinsic type cudaError_t used
#define __TYPE_DECL_cudaError_t__
#if defined(__cplusplus) || defined(__CUDACC__)
#ifndef __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__
#define __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__
namespace Hybridizer { namespace Runtime { namespace CUDAImports { 
enum struct ResidentArrayStatus
{
	NoAction = 0, 
	DeviceNeedsRefresh = 1, 
	HostNeedsRefresh = 2, 
} ;
} } } // Leaving namespace
#endif // __ENUM_DECL_Hybridizer_Runtime_CUDAImports_ResidentArrayStatus__

namespace Hybridizer { namespace Runtime { namespace CUDAImports { 
struct IResidentArray ;
} } } // Leaving namespace
namespace Hybridizer { namespace Runtime { namespace CUDAImports { 
struct IResidentData ;
} } } // Leaving namespace
namespace System { 
struct IDisposable ;
} // Leaving namespace
// Intrinsic type cudaError_t used
#define __TYPE_DECL_cudaError_t__
namespace Hybridizer { namespace Runtime { namespace CUDAImports { 
struct IntResidentArray ;
} } } // Leaving namespace
namespace Hybrid { 
struct Program ;
} // Leaving namespace
namespace Hybrid { 
struct Program___c__DisplayClass1_0 ;
} // Leaving namespace
namespace System { namespace Threading { namespace Tasks { 
struct Parallel ;
} } } // Leaving namespace
// Intrinsic type Nullable`1 used
#define __TYPE_DECL_hybridizer_nullable__int64_t____
namespace System { namespace Threading { namespace Tasks { 
struct ParallelLoopResult ;
} } } // Leaving namespace
// Intrinsic type Action`1 used
#define __TYPE_DECL_hybridizer_action__int____
namespace Hybrid { 
struct Program___c__DisplayClass2_0 ;
} // Leaving namespace
namespace Hybrid { 
struct Program___c__DisplayClass3_0 ;
} // Leaving namespace
namespace Hybrid { 
struct Program___c__DisplayClass7_0 ;
} // Leaving namespace
#endif // TOTO
#pragma endregion

extern "C" void* __hybridizer_init_basic_runtime();
// -----  HYBRIDIZER_CUDA_CUH ----- 
#pragma once // hybridizer.cuda.cuh

#if defined(__CUDACC_RTC__)
#define HYBRIDIZER_NO_HOST
#endif


#if !defined(HYBRIDIZER_NO_HOST)
#include <cuda.h>
#include <stdint.h>
#include <stdio.h>
#else
typedef signed char        int8_t;
typedef short              int16_t;
typedef int                int32_t;
typedef long long          int64_t;
typedef unsigned char      uint8_t;
typedef unsigned short     uint16_t;
typedef unsigned int       uint32_t;
typedef unsigned long long uint64_t;
#endif



#if !defined(DLL_PUBLIC)
#if defined _WIN32 || defined _WIN64 || defined __CYGWIN__
  #define BUILDING_DLL
  #ifdef BUILDING_DLL
    #ifdef __GNUC__
      #define DLL_PUBLIC __attribute__ ((dllexport))
    #else
      #define DLL_PUBLIC __declspec(dllexport) // Note: actually gcc seems to also supports this syntax.
    #endif
  #else
    #ifdef __GNUC__
      #define DLL_PUBLIC __attribute__ ((dllimport))
    #else
      #define DLL_PUBLIC __declspec(dllimport) // Note: actually gcc seems to also supports this syntax.
    #endif
  #endif
  #define DLL_LOCAL
#else
  #if __GNUC__ >= 4
    #define DLL_PUBLIC __attribute__ ((visibility ("default")))
    #define DLL_LOCAL  __attribute__ ((visibility ("hidden")))
  #else
    #define DLL_PUBLIC
    #define DLL_LOCAL
  #endif
#endif
#endif

#if UINTPTR_MAX == 0xffffffffffffffff
/* 64-bit */
#else
// WTF ??? #define NO_EXCEPTION
#endif

#define HYBRIDIZER_GETTYPE_ID hybridizer::gettypeid
template<typename T> struct __hybridizer_argument_type;
template<typename T, typename U> struct __hybridizer_argument_type<T(U)> { typedef U type; };
#define HYBRIDIZER_QUERYINTERFACE(...) hybridizer::queryinterface<__hybridizer_argument_type<void(__VA_ARGS__)>::type>
#define HYBRIDIZER_SIGNAL_PURE_VIRTUAL(a, b) hybridizer::signal_pure_virtual(a, b)

#define HYBRIDIZER_FUNC3_INVOKE(functor,left,middle,right) (*functor) . invoke (left, middle, right) 
#define HYBRIDIZER_FUNC2_INVOKE(functor,left,right) (*functor) . invoke (left, right) 
#define HYBRIDIZER_FUNC1_INVOKE(functor,left) (*functor) . invoke (left) 


#if defined(__CUDACC__)
	#ifndef hyb_device
		#define hyb_inline __forceinline__ 
		
		#define hyb_constant __constant__
		#if defined(HYBRIDIZER_NO_HOST)
			#define hyb_host
			#define	hyb_device  __device__  
		#else
			#define hyb_host __host__
			#define	hyb_device  __device__   
		#endif
	#endif
#else
	#ifndef hyb_device
		#define hyb_inline inline 
		#define hyb_device
		#define hyb_constant 
	#endif
#endif

#if defined(_MSC_VER)
#define STRUCT_ALIGNED_(x) __declspec(align(x))
#else
#if defined(__GNUC__)
#define STRUCT_ALIGNED_(x) __attribute__ ((aligned(x)))
#endif
#endif

#define __hybridizer_threadIdxX threadIdx.x
#define __hybridizer_threadIdxY threadIdx.y
#define __hybridizer_threadIdxZ threadIdx.z
#define __hybridizer_blockDimX blockDim.x
#define __hybridizer_blockDimY blockDim.y
#define __hybridizer_blockDimZ blockDim.z
#define __hybridizer_blockIdxX blockIdx.x
#define __hybridizer_blockIdxY blockIdx.y
#define __hybridizer_blockIdxZ blockIdx.z
#define __hybridizer_gridDimX gridDim.x
#define __hybridizer_gridDimY gridDim.y
#define __hybridizer_gridDimZ gridDim.z

#define __hybridizer_threadIdxXX64 threadIdx.x
#define __hybridizer_threadIdxYX64 threadIdx.y
#define __hybridizer_threadIdxZX64 threadIdx.z
#define __hybridizer_blockDimXX64 blockDim.x
#define __hybridizer_blockDimYX64 blockDim.y
#define __hybridizer_blockDimZX64 blockDim.z
#define __hybridizer_blockIdxXX64 blockIdx.x
#define __hybridizer_blockIdxYX64 blockIdx.y
#define __hybridizer_blockIdxZX64 blockIdx.z
#define __hybridizer_gridDimXX64 gridDim.x
#define __hybridizer_gridDimYX64 gridDim.y
#define __hybridizer_gridDimZX64 gridDim.z

extern __shared__ char __hybridizer_cuda_local_shared [] ;

#if defined(HYBRIDIZER_NULL_CHECKS_THROW_TRAP) || defined (HYBRIDIZER_NULL_CHECKS_BREAK) || defined (HYBRIDIZER_NULL_CHECKS_PRINT)
#define HYBRIDIZER_NULL_CHECKS
#endif

#ifdef HYBRIDIZER_NULL_CHECKS
template<typename T>
hyb_device hyb_inline static T* __hybridizer_null_check(T* input, const char* file, int line) {
	if(nullptr == input) {
#ifdef HYBRIDIZER_NULL_CHECKS_BREAK
		asm("brkpt;");
#elif defined (HYBRIDIZER_NULL_CHECKS_THROW_TRAP)
		asm("trap;");
#elif defined(HYBRIDIZER_NULL_CHECKS_PRINT)
		printf("null pointer at %s:%d\n", file, line);
#else 
#endif
	}
	return input;
}
#define HYBRIDIZER_NULL_CHECK(param) __hybridizer_null_check(param, __FILE__, __LINE__)
#else 
#define HYBRIDIZER_NULL_CHECK(param) (param)
#endif

namespace hybridizer { 
	template <typename T> __device__ T shuffle (T t, int srcLane) { return __shfl (t, srcLane) ; }
	template <typename T> __device__ T shuffleup (T t, unsigned int shift) { return __shfl_up (t, shift) ; }
	template <typename T> __device__ T shuffledown (T t, unsigned int shift) { return __shfl_down (t, shift) ; }
	template <typename T> __device__ T shufflexor (T t, unsigned int shift) { return __shfl_xor (t, shift) ; }
}

namespace hybridizer {
	template <typename T, int rank>
	struct hybarray;
}

namespace hybridizer {
	struct hybridobject { union { void*    _vtable ; int _typeid ; char _vtable_padding[8] ; } ; } ;
	struct datetime { long long _date; } ;

	__forceinline__ hyb_device static int gettypeid (void* ptr) {
		if (ptr == 0) return 0 ;
		return ((hybridobject*)ptr)->_typeid ;
	}

	template<typename T>
	__forceinline__ hyb_device static T queryinterface (hybridobject* ptr) {
		return ((T)ptr) ;
	}
	template<typename T>
	__forceinline__ hyb_device static T queryinterface (void* ptr) {
		return ((T)ptr) ;
	}
	template<typename T>
	hyb_inline hyb_device static T constrained (T* ptr) { return *ptr; }
	template<typename T>
	hyb_inline hyb_device static T constrained (T const * ptr) { return *ptr; }
	template<typename T>
	hyb_inline hyb_device static T constrained (T& ptr) { return ptr; }
	template<typename T>
	hyb_inline hyb_device static T constrained (T const & ptr) { return ptr; }

	template<typename T>
	struct sharedmemoryallocator
	{
		// TODO : HYB-794 => shared max allocation => error !!
		// Allocate a hybarray in shared memory (can be automatically converted to raw pointer if needed)
		__forceinline__ __device__ hybridizer::hybarray<T, 1> allocate(int count)
		{
			hybridizer::hybarray<T, 1> res ;
			res.ptr = (T*) (&(__hybridizer_cuda_local_shared[(*((int*)__hybridizer_cuda_local_shared))])) ;
			res.length[0] = count;
			res.lowerBound[0] = 0;
			__syncthreads();
			if (threadIdx.x == 0 && threadIdx.y == 0) {
				int sizeElt = sizeof(T);
				int size = count * sizeElt;
				(*((int*)__hybridizer_cuda_local_shared)) += size ;
			}
			return res ;
		}
		
		// Allocate a raw pointer in shared memory
		__forceinline__ __device__ T* allocate_raw(int count)
		{
			T* res = (T*) (&(__hybridizer_cuda_local_shared[(*((int*)__hybridizer_cuda_local_shared))])) ;
			__syncthreads();
			if (threadIdx.x == 0 && threadIdx.y == 0) {
				(*((int*)__hybridizer_cuda_local_shared)) += count * sizeof (T) ;
			}

			return res ;
		}
		int initialOffset ;
		bool resetAtDestruction;

		__forceinline__ __device__ sharedmemoryallocator(bool reset = true)
		{
			initialOffset = (*((int*)__hybridizer_cuda_local_shared)) ;
			resetAtDestruction = reset;
		}

		__forceinline__ __device__ ~sharedmemoryallocator()
		{
			__syncthreads();
			if (threadIdx.x == 0 && threadIdx.y == 0 && resetAtDestruction) {
				if ((*((int*)__hybridizer_cuda_local_shared)) > initialOffset)
					*((int*)__hybridizer_cuda_local_shared) = initialOffset;
			}
		}
	} ;

	hyb_inline hyb_device double getarraydouble(void* unused, void* ar, int idx) { return ((double*)ar)[idx] ; }
	hyb_inline hyb_device void setarraydouble(void* unused, void* ar, int idx, double value) { ((double*)ar)[idx] = value ; }

	hyb_inline hyb_device float getarrayfloat(void* unused, void* ar, int idx) { return ((float*)ar)[idx] ; }
	hyb_inline hyb_device void setarrayfloat(void* unused, void* ar, int idx, float value) { ((float*)ar)[idx] = value ; }
	
	hyb_inline hyb_device float getarrayint(void* unused, void* ar, int idx) { return ((int*)ar)[idx] ; }
	hyb_inline hyb_device void setarrayint(void* unused, void* ar, int idx, float value) { ((int*)ar)[idx] = value ; }

	template<typename T> hyb_inline hyb_device T getarray(void* unused, void* ar, int idx) { return ((T*)ar)[idx] ; }
	template<typename T> hyb_inline hyb_device void setarray(void* unused, void* ar, int idx, T value) { ((T*)ar)[idx] = value ; }
}

__forceinline__ __device__ char* SharedMemoryPointer(int count) 
{
	hybridizer::sharedmemoryallocator<char> allocator;
	allocator.resetAtDestruction = false;
	return allocator.allocate_raw(count);
}

__forceinline__ __device__ char* GetSharedMemoryArray() {
	return (char*) __hybridizer_cuda_local_shared;
}
/*
namespace hybridizer 
{
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) ()) 
	{
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>();
	}

	template <typename T>
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) (T arg), T a) 
	{
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>(a);
	}

	template <typename T1, typename T2>
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) (T1 arg1, T2 arg2), T1 a, T2 b) {
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>(a, b);
	}

	template <typename T1, typename T2, typename T3>
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) (T1 arg1, T2 arg2, T3 arg3), T1 a, T2 b, T3 c) {
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>(a, b, c);
	}

	template <typename T1, typename T2, typename T3, typename T4>
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) (T1 arg1, T2 arg2, T3 arg3, T4 arg4), T1 a, T2 b, T3 c, T4 d) {
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>(a, b, c, d);
	}

	template <typename T1, typename T2, typename T3, typename T4, typename T5>
	void launch_kernel(int griddimx, int griddimy, int blockdimx, int blockdimy, int blockdimz, int shared, 
					   void (*kernel) (T1 arg1, T2 arg2, T3 arg3, T4 arg4, T5 arg5), T1 a, T2 b, T3 c, T4 d, T5 e) {
		dim3 grid (griddimx, griddimy, 1);
		dim3 block(blockdimx, blockdimy, blockdimz);
		kernel<<<grid, block, shared>>>(a, b, c, d, e);
	}
	
	// TODO: more versions or variadic templates
}
*/

/*
template <> __device__ int hybridizer::constrained<int,int*>(int* i) { return *i ; }
template <> __device__ int hybridizer::constrained<int,int>(int& i) { return i ; }
*/
#if !defined(__CUDA_ARCH__)
#include <limits>
#endif

#ifndef HYBRIDIZER_NO_HOST
#include <stdio.h>
#include <stdarg.h>

#define hybridizer__hyprintfva(f, format, ...) printf (format, __VA_ARGS__)
#define hybridizer__hyprintflineva(f, format, ...) hybridizer__hyprintfva (f, format "\n", __VA_ARGS__)

namespace hybridizer
{
	inline __device__ static FILE* get_Out() {
		return 0 ;
	}
	inline __device__ static void hyprintf(FILE* f, const char* message) {
		printf(message);
	}
	inline __device__ static void hyprintfline(FILE* f, const char* message) {
		printf("%s\n", message);
	}
};

#include <math_constants.h>
namespace hybridizer
{
	#if defined(__CUDA_ARCH__)
	template <typename T> __device__ hyb_host T nan () ; 
	template <typename T> __device__ hyb_host T pos_infinity () ; 
	template <typename T> __device__ hyb_host T neg_infinity () ; 
	
	template <> __forceinline__ __device__ float nan<> () { return CUDART_NAN_F ; }
	template <> __forceinline__ __device__ float pos_infinity<> () { return CUDART_INF_F ; } 
	template <> __forceinline__ __device__ float neg_infinity<> () { return - CUDART_INF_F ; }
	template <> __forceinline__ __device__ double nan<> () { return CUDART_NAN ; }
	template <> __forceinline__ __device__ double pos_infinity<> () { return CUDART_INF ; }
	template <> __forceinline__ __device__ double neg_infinity<> () { return - CUDART_INF ; }
	#endif

	#if !defined(__CUDA_ARCH__)
	template <typename T> __device__ T nan () { return std::numeric_limits<T>::quiet_NaN(); }
	template <typename T> __device__ T pos_infinity () { return std::numeric_limits<T>::infinity(); }
	template <typename T> __device__ T neg_infinity () { return - std::numeric_limits<T>::infinity(); }
	#endif
};

#else
// HYBRIDIZER_NO_HOST

#define hybridizer__hyprintfva(f, format, ...) printf (format, __VA_ARGS__)
#define hybridizer__hyprintflineva(f, format, ...) hybridizer__hyprintfva (f, format "\n", __VA_ARGS__)

typedef void FILE ;

namespace hybridizer
{
	inline __device__ static FILE* get_Out() {
		return 0 ;
	}
	inline __device__ static void hyprintf(FILE* f, const char* message) {
		printf(message);
	}
	inline __device__ static void hyprintfline(FILE* f, const char* message) {
		printf("%s\n", message);
	}
};

// for NVRTC
namespace hybridizer
{
	#if defined(__CUDA_ARCH__)
	template <typename T> __device__ hyb_host T nan () ; 
	template <typename T> __device__ hyb_host T pos_infinity () ; 
	template <typename T> __device__ hyb_host T neg_infinity () ; 
	
	template <> __forceinline__ __device__ float nan<> () { return __int_as_float(0x7fffffff); }
	template <> __forceinline__ __device__ float pos_infinity<> () { return __int_as_float(0x7f800000) ; } 
	template <> __forceinline__ __device__ float neg_infinity<> () { return - __int_as_float(0x7f800000) ; }
	template <> __forceinline__ __device__ double nan<> () { return __longlong_as_double(0xfff8000000000000ULL) ; }
	template <> __forceinline__ __device__ double pos_infinity<> () { return __longlong_as_double(0x7ff0000000000000ULL) ; }
	template <> __forceinline__ __device__ double neg_infinity<> () { return - __longlong_as_double(0x7ff0000000000000ULL) ; }
	#endif
}

#endif



namespace hybridizer
{
	__device__ static void signal_pure_virtual (const char* method, int id)
	{
		// TODO : throw exception...
		::printf ("Pure virtual call for method <%s> on type id <%d> (CUDA flavor)\n", method, id) ;
	}
};
namespace hybridizer
{
	struct runtime;

	__forceinline__ __device__ void initruntime(hybridizer::runtime* rt)
	{
		if (threadIdx.x == 0 && threadIdx.y == 0)
		{
			// init shared memory offset to zero
			(*((int*)__hybridizer_cuda_local_shared)) = 16 ; 
			(*(((int*)__hybridizer_cuda_local_shared) + 1)) = 0 ; 
#ifndef NO_EXCEPTION
			(*((runtime**)(__hybridizer_cuda_local_shared + 8))) = rt ;
#endif
		}
		__syncthreads();
	}
	__forceinline__ __device__ hybridizer::runtime* getruntime()
	{
#ifndef NO_EXCEPTION
		return (*((runtime**)(__hybridizer_cuda_local_shared + 8))) ;
#else
		return 0;
#endif
	}
}

#ifndef NO_EXCEPTION
#pragma once

//#define HYBRIDIZER_EXCEPTIONS_HANDLE_THREAD
#include <cuda_runtime_api.h>
#define CUDA_CHECK(x) {if (x != cudaSuccess) {printf("Cuda error %d\n", x); exit(-1);} }

// EXCEPTION MODES
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
#define HYBRIDIZER_EXCEPTIONS_MODE_DEFINED
#endif

#ifdef HYBRIDIZER_EXCEPTIONS_HANDLE_THREAD
#define HYBRIDIZER_EXCEPTIONS_MODE_DEFINED
#endif

#ifdef HYBRIDIZER_EXCEPTIONS_HANDLE_BLOCK
#define HYBRIDIZER_EXCEPTIONS_MODE_DEFINED
#endif

#ifdef HYBRIDIZER_EXCEPTIONS_NONE
#define NO_EXCEPTION
#endif

#ifdef NO_EXCEPTION
#define HYBRIDIZER_EXCEPTIONS_MODE_DEFINED
#endif

#ifndef HYBRIDIZER_EXCEPTIONS_MODE_DEFINED
#define NO_EXCEPTION
	#endif

#pragma region Runtime : Exceptions management

#ifndef __FILE_ID__ 
#define __FILE_ID__ 42
#endif


#pragma region Bit scan forward intrinsics
#if defined(_MSC_VER) && !defined(__clang__)
	#include <intrin.h>
	#pragma intrinsic(_BitScanForward)
	static hyb_inline  uint32_t bit_scan_forward(uint32_t x) 
	{
		unsigned long res;
		int status = _BitScanForward(&res, x);
		return status ? res + 1 : 0;
	}
	extern void _hyb_raise_exception(int errorCode);

#elif defined (__INTEL_COMPILER)
	static hyb_inline  uint32_t bit_scan_forward(uint32_t x) 
	{
		int r;
		__asm__("bsf %1,%0" : "=r"(r) : "X"(x));
		return r;
	}
#elif defined(__clang__)
	static hyb_inline  uint32_t bit_scan_forward(uint32_t x) 
	{
		return (uint32_t)__builtin_ffs (x) ;
	}
#elif defined(__GNUC__)
	static hyb_inline  uint32_t bit_scan_forward(uint32_t x) 
	{
		return (uint32_t)__builtin_ffs (x) ;
	}
#else
	#error "Unsupported compiler - no ffs intrinsic"
#endif
#pragma endregion

#ifndef NO_EXCEPTION
#include <thread>
#include <map>
#include <driver_types.h>
#include <mutex>
#endif

namespace hybridizer
{
	struct exceptionframe
	{
		int filenum;
		int linenum;
	};

	struct exceptionentry
	{
		union { void* exceptiondata; exceptionframe frame; };

		hyb_host hyb_device hyb_inline exceptionentry(int file, int line)
		{
			frame.filenum = file;
			frame.linenum = line;
		}

		hyb_host hyb_device hyb_inline exceptionentry(void* data)
		{
			exceptiondata = data;
		}
	};

	struct exceptioninstance
	{
		union { void* __vtable; int __typeid; };
	};

	struct exceptionstack
	{
		exceptionentry entries[1023];
		int count;
		int location;
		int code;

		hyb_device hyb_inline void set(exceptionentry entry, int loc, int code)
		{
			count = 1;
			entries[0] = entry;
			location = loc;
			this->code = code;
		}

		hyb_device hyb_inline void populate(int file, int line)
		{
			entries[count].frame.filenum = file;
			entries[count].frame.linenum = line;
			++count;
			if (count == 1023) count = 1022; // skip the topmost frames...
		}
	};

	struct baseexception
	{
		hyb_device hyb_inline static int hybridizer_typeid() {return 0;}
		union { void* __vtable; int __typeid; };
		union { char* _message; char __message_padding[8]; };
	};

	struct gpuexception : baseexception
	{
	};

	struct nullpointerexception : baseexception {};
	struct indexoutofboundsexception : baseexception {};

	template <typename T>
	struct exceptionhandle
	{
		T exception;
		exceptionentry* stack; // this is only valid until the next throw...
		int stacksize;

		hyb_device hyb_inline exceptionhandle() {}
		hyb_device hyb_inline exceptionhandle(T ex) { exception = ex; }
	};

	class runtime
	{
	public:
		typedef void (* exception_callback_type)(const int code);

#ifndef HYBRIDIZER_NO_HOST
		hyb_inline hyb_host static runtime* host_getruntime(cudaStream_t stream);
		hyb_inline hyb_host static runtime* host_getruntime();
		hyb_inline hyb_host void host_release();

		hyb_inline hyb_host static void hostrethrow(runtime* hrt);
#endif

		hyb_device hyb_inline static void init(runtime* rt);

		template<typename extype> 
		hyb_device hyb_inline static void* allocateinstance()
		{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			return nullptr;
#else
			runtime* local_runtime = (runtime*)getruntime();
			int index = atomicAdd(&(local_runtime->exceptioninstancescount), sizeof(extype));
			return (void*)(local_runtime->exceptioninstances + index);
#endif
#else
			return NULL;
#endif
		}

		int exceptionstatus[64]; // only valid in shared memory
		exceptionstack* exceptionstacks; // One stack per thread in grid (can be big)
		unsigned exceptioncount;
		unsigned exceptioninstancescount;
		unsigned gridsize; // Currently allocated exceptionstacks

		int* exceptionentries;
		char* exceptioninstances;
		exception_callback_type _exception_callback;

	private:
		cudaStream_t _stream;
		runtime(cudaStream_t stream) : _stream(stream) {}
#ifndef NO_EXCEPTION
		static std::map<std::pair<std::thread::id, cudaStream_t>, runtime*> g_runtime_dict;
		static std::mutex g_dict_mutex;
#endif

	public:
		hyb_device hyb_inline static int threadid()
		{
			return threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);
		}
		hyb_device hyb_inline static int blockid()
		{
			return blockIdx.x + gridDim.x * (blockIdx.y + gridDim.y * blockIdx.z);
		}

		hyb_device hyb_inline static bool inexception()
		{
#ifndef NO_EXCEPTION
			//if (reinterpret_cast<int*>(__hybridizer_cuda_local_shared)[1] == 0)
			//	return false;
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			return false;
#else

			runtime* local_runtime = (runtime*)getruntime();
			int tid = threadid();
			__syncthreads();
			int warpStatus = local_runtime->exceptionstatus[tid >> 5];
			int mask = 1 << (tid & 31);
			return (warpStatus & mask) != 0;
#endif
#else
			return false;
#endif
		}

		hyb_device hyb_inline static void throwexception(void* data, int file, int line, int code)
		{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			asm("trap;");
#else
			//reinterpret_cast<int*>(__hybridizer_cuda_local_shared)[1] = -1;

			runtime* local_runtime = (runtime*)getruntime();
			int tid = threadid();
			int bid = blockid();
			atomicOr(&(local_runtime->exceptionstatus[tid >> 5]), 1 << (tid & 31));
			int index = tid + (blockDim.x * blockDim.y * blockDim.z * bid);
			unsigned int location = atomicInc(&local_runtime->exceptioncount, 0x7FFFFFFF);
			local_runtime->exceptionstacks[index].set(exceptionentry(data), location, code);
			local_runtime->exceptionstacks[index].populate(file, line);
			//local_runtime->exceptionentries [location] = index ;
#endif
#endif
		}

		hyb_device hyb_inline static void populatestackframe(int file, int line)
		{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			// DO NOTHING
#else
			if (inexception())
			{
				runtime* local_runtime = (runtime*)getruntime();
				int tid = threadid();
				int bid = blockid();
				int index = tid + (blockDim.x * blockDim.y * blockDim.z * bid);
				local_runtime->exceptionstacks[index].populate(file, line);
			}
#endif
#endif
		}

		// returns true if the instancetypeid is a subtype of basetypeid
		hyb_device hyb_inline static bool implements(int basetypeid, int instancetypeid)
		{
			// TODO : really work
			return basetypeid == instancetypeid;
		}



		template <typename T>
		hyb_device hyb_inline static exceptionhandle<T> catchexception()
		{
			exceptionhandle<T> result;
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			return result;
#else
			if (!inexception()) return exceptionhandle<T>(0);
			runtime* local_runtime = (runtime*)getruntime();
			int tid = threadid();
			int bid = blockid();
			int index = tid + (blockDim.x * blockDim.y * blockDim.z * bid);
			baseexception* be = (baseexception*)local_runtime->exceptionstacks[index].entries[0].exceptiondata;
			// if (be == 0) return exceptionhandle<T>(0) ; // => ideally, actually return an inner runtime exception
			// TODO :  if (runtime::implements (T::extypeid(), be->__typeid))
			{
				// exception is caught - get the call stack....
				result.exception = (T)be;
				result.stacksize = local_runtime->exceptionstacks[index].count - 1;
				result.stack = local_runtime->exceptionstacks[index].entries + 1;
				// ... and reset !
				int location = local_runtime->exceptionstacks[index].location;
				//local_runtime->exceptionentries [location] = -1 ; // reset 
				local_runtime->exceptionstacks[index].count = 0;
				int mask = ~(1 << (tid & 31));
				auto tmp = local_runtime->exceptionstatus[tid >> 5];
				atomicAnd(&(local_runtime->exceptionstatus[tid >> 5]), ~(1 << (tid & 31)));
			}
			// see TODO : else {
			// exception is not in proper type hierarchy
			//	return exceptionhandle<T>(0) ;
			//}
#endif
#endif
			return result;
		}

#ifndef HYBRIDIZER_NO_HOST
		static hyb_inline void hostfree(runtime* hrt)
		{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			// DO NOTHING
#else
			::cudaFree(hrt);
#endif
#endif
		}

		static hyb_inline void hostinit(runtime* hrt, int gridsize, int exbuffersize)
		{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			// DO NOTHING
#else
			// TODO : check and report this error !!!
			for (int k = 0; k < 64; ++k)
			{
				hrt->exceptionstatus[k] = 0;
			}
			hrt->exceptioncount = 0;
			hrt->exceptioninstancescount = 0;

			CUDA_CHECK(::cudaMallocManaged(&(hrt->exceptionstacks), gridsize * sizeof(exceptionstack) + exbuffersize));
			hrt->exceptioninstances = (((char*)hrt->exceptionstacks) + gridsize * sizeof(exceptionstack));
			hrt->gridsize = gridsize;
#endif
#endif
		}
	};
		
	void runtime::hostrethrow(runtime* hrt)
	{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
		// DO NOTHING
#else
		for (int k = 0; k < 64; ++k)
		{
			if (hrt->exceptionstatus[k] != 0)
			{
				int idx = k * 64 + bit_scan_forward(hrt->exceptionstatus[k]);
				// we have an exception => throw
				printf("Throwing %d\n", hrt->exceptionstacks[idx].code);
				//throw hrt->exceptionstacks[idx].code;
				hrt->_exception_callback(hrt->exceptionstacks[idx].code);
			}
		}
		hrt->host_release();
#endif
#endif
	}
#endif

	hyb_device void runtime::init(runtime* rt)
	{
#ifndef NO_EXCEPTION
#ifdef HYBRIDIZER_EXCEPTIONS_THROWTRAP
			// DO NOTHING !
#else
			runtime* local_runtime = getruntime();
			int tid = threadid();
			if ((tid & 31) == 0) local_runtime->exceptionstatus[tid >> 5] = 0;
			__syncthreads();
#endif
#endif
	}

	#ifndef HYBRIDIZER_NO_HOST
	hyb_host runtime* runtime::host_getruntime(cudaStream_t stream)
	{
#ifdef NO_EXCEPTION
		return nullptr;
#else
		auto key = std::make_pair(std::this_thread::get_id(), stream);

		std::lock_guard<std::mutex> guard(g_dict_mutex);
		{
			auto it = g_runtime_dict.find(key);
			runtime* res;
			if (it == g_runtime_dict.end())
			{
				CUDA_CHECK(::cudaMallocManaged((void**)&res, sizeof(runtime)));
				res->_stream = stream;
				g_runtime_dict[key] = res;
			} else 
				res = it->second;
			return res;			
		}
#endif
	}

	hyb_host runtime* runtime::host_getruntime()
	{
		return host_getruntime((cudaStream_t) 0);
	}

	hyb_host void runtime::host_release()
	{
#ifndef NO_EXCEPTION
		auto key = std::make_pair(std::this_thread::get_id(), _stream);

		std::lock_guard<std::mutex> guard(g_dict_mutex);
		{
			auto it = g_runtime_dict.find(key);
			if (it != g_runtime_dict.end())
				g_runtime_dict.erase(key);

			::cudaFree(this->exceptionstacks);
			::cudaFree(this);
		}
#endif
	}
	#endif
}

#pragma endregion


#ifndef NO_EXCEPTION

hyb_device hyb_inline void thrownullpointerexception(int fileid, int line)
{
	::hybridizer::runtime::throwexception(::hybridizer::runtime::allocateinstance<::hybridizer::nullpointerexception>(), fileid, line, -1);
}
hyb_device hyb_inline void throwindexoutofboundsexception(int fileid, int line)
{
	::hybridizer::runtime::throwexception(::hybridizer::runtime::allocateinstance<::hybridizer::indexoutofboundsexception>(), fileid, line, -2);
}

namespace hybridizer
{
	template <typename T, typename I>
	hyb_device hyb_inline const T& checknpeandbounds(const hybarray<T,1>& a, I i, int fileid, int line)
	{
		if (a.ptr == nullptr) thrownullpointerexception(fileid, line);
		if (i < a.lowerBound[0]) throwindexoutofboundsexception(fileid, line);
		if (i >= a.length[0]) throwindexoutofboundsexception(fileid, line);
		return a [i] ;
	}

	template <typename T, typename I>
	hyb_device hyb_inline T& checknpeandbounds(hybarray<T, 1>& a, I i, int fileid, int line)
	{
		if (a.ptr == nullptr) thrownullpointerexception(fileid, line);
		if (i < a.lowerBound[0]) throwindexoutofboundsexception(fileid, line);
		if (i >= a.length[0]) throwindexoutofboundsexception(fileid, line);
		return a[i];
	}

	template <typename T>
	hyb_device hyb_inline T* checknpe(T* p, int fileid, int line)
	{
		if (p == nullptr) thrownullpointerexception(fileid, line);
		return p ;
	}

	template <typename T, int rank>
	hyb_device hyb_inline hybarray<T, rank> checknpe(hybarray<T, rank> p, int fileid, int line)
	{
		if (p.ptr == nullptr) thrownullpointerexception(fileid, line);
		return p ;
	}
}

#define __hybridizer__checknpeandbounds(a,i) hybridizer::checknpeandbounds(a,i,__FILE_ID__, __LINE__)
#define __hybridizer__checknpe(a) hybridizer::checknpe(a,__FILE_ID__, __LINE__)

#else

#define __hybridizer__checknpeandbounds(a,i) a[i]
#define __hybridizer__checknpe(a) a


#endif

#endif


namespace hybridizer {
	// Native structure for array with length, can be used as an array anywhere
	template <typename T, int rank>
	struct hybarray {
		union {T *ptr ; char __hybridizer_padding_ptr[8] ; } ;
		uint32_t length[rank] ; 
		uint32_t lowerBound[rank];

		hyb_inline hyb_device bool check_index(int k, int i) const
		{
#ifndef NO_EXCEPTION
			if (i < 0 || i >= length[k])
			{
				::hybridizer::runtime::throwexception(::hybridizer::runtime::allocateinstance<::hybridizer::baseexception>(), __FILE_ID__, __LINE__, -2) ;
				return false;
			}
#endif			
			return true;
		}
		hyb_inline hyb_device const T& operator[] (int i) const { if (check_index(0, i)) return ptr [i];}
		hyb_inline hyb_device T& operator[] (int i) { if (check_index(0, i)) return ptr [i];}
		hyb_inline hyb_device operator T* () const { return ptr ; }
		hyb_inline hyb_device int getRank() const { return rank ; }
		hyb_inline hyb_device int getLength(int d) const { return length[d] ; }
		hyb_inline hyb_device int getLowerbound(int d) const { return lowerBound[d] ; }

		hyb_inline hyb_device int getIndex(const int indexes[]) const
		{
			int index = 0;
			for (int k = 0 ; k < rank ; ++k)
			{
				int sub = indexes[k] - lowerBound[k];
				if (k > 0) index *= length [k];
				index += sub ;
				check_index(k, sub);
			}
			return index;
		}

		hyb_inline hyb_device const T& get(const int indexes[]) const
		{
             return ptr [getIndex(indexes)] ;
		}

		hyb_inline hyb_device T* getAdr(const int indexes[]) const
		{
             return ptr + getIndex(indexes) ;
		}

		hyb_inline hyb_device void set(const int indexes[], T value)
		{
			 ptr[getIndex(indexes)] = value;
		}
	};

	template <typename T, int rank> hyb_inline hyb_device int hyblength(hybarray<T, rank> ar, int dim) { return ar.length[dim] ; }
	template <typename T, int rank> hyb_inline hyb_device int hyblength(hybarray<T, rank>* ar, int dim) { return ar->length[dim] ; }
	template <typename T, int rank> hyb_inline hyb_device int hyblowerbound(hybarray<T, rank> ar, int dim) { return ar.lowerBound[dim] ; }
	template <typename T, int rank> hyb_inline hyb_device int hyblowerbound(hybarray<T, rank>* ar, int dim) { return ar->lowerBound[dim] ; }
	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank> ar, int i) { return ar[i] ; }
	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank> ar, int i, int j) { int idx[2] = {i, j}; return ar.get(idx) ; }
	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank> ar, int i, int j, int k) { int idx[3] = {i, j, k}; return ar.get(idx) ; }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank> ar, int i, T value) { return ar[i] = value ; }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank> ar, int i, int j, T value) { int idx[2] = {i, j}; return ar.set(idx, value) ; }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank> ar, int i, int j, int k, T value) { int idx[3] = {i, j, k}; return ar.set(idx, value) ; }

	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank>* ar, int i) { return ar->operator [](i); }
	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank>* ar, int i, int j) { int idx[2] = { i, j }; return ar->get(idx); }
	template <typename T, int rank> hyb_inline hyb_device const T& hybget(hybarray<T, rank>* ar, int i, int j, int k) { int idx[3] = { i, j, k }; return ar->get(idx); }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank>* ar, int i, T value) { return ar->operator [](i) = value; }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank>* ar, int i, int j, T value) { int idx[2] = { i, j }; return ar->set(idx, value); }
	template <typename T, int rank> hyb_inline hyb_device void hybset(hybarray<T, rank>* ar, int i, int j, int k, T value) { int idx[3] = { i, j, k }; return ar->set(idx, value); }

	template <typename T, int rank> hyb_inline hyb_device T* hybaddress(hybarray<T, rank> ar, int i) { return ar + i ; }
	template <typename T, int rank> hyb_inline hyb_device T* hybaddress(hybarray<T, rank> ar, int i, int j) { int idx[2] = {i, j}; return ar.getAdr(idx) ; }
	template <typename T, int rank> hyb_inline hyb_device T* hybaddress(hybarray<T, rank> ar, int i, int j, int k) { int idx[3] = {i, j, k}; return ar.getAdr(idx) ; }
};

/* surface management */
#pragma region Surface Management

/*#if (__CUDA_ARCH__ < 300)

#error __CUDA_ARCH__ < 300

#else */
#if 1
namespace hybridizer
{
	#if (__CUDA_ARCH__ >= 300)
	template <int size> inline __device__ void sizedSurf2Dread (cudaSurfaceObject_t surface, int x, int y, void* output) ;
	template<> inline __device__ void sizedSurf2Dread<16>(cudaSurfaceObject_t surface, int x, int y, void* output)
	{
		int4 tmp = ::surf2Dread<int4>(surface, x * 16, y) ;
		((int*)output)[0] = tmp.x ;
		((int*)output)[1] = tmp.y ;
		((int*)output)[2] = tmp.z ;
		((int*)output)[3] = tmp.w ;
	}
	template<> inline __device__ void sizedSurf2Dread<8>(cudaSurfaceObject_t surface, int x, int y, void* output)
	{
		int2 tmp = ::surf2Dread<int2>(surface, x * 8, y) ;
		((int*)output)[0] = tmp.x ;
		((int*)output)[1] = tmp.y ;
	}
	template<> inline __device__ void sizedSurf2Dread<4>(cudaSurfaceObject_t surface, int x, int y, void* output)
	{
		int tmp = ::surf2Dread<int>(surface, x * 4, y) ;
		((int*)output)[0] = tmp ;
	}
	template<> inline __device__ void sizedSurf2Dread<2>(cudaSurfaceObject_t surface, int x, int y, void* output)
	{
		short tmp = ::surf2Dread<short>(surface, x * 2, y) ;
		((short*)output)[0] = tmp ;
	}
	template<> inline __device__ void sizedSurf2Dread<1>(cudaSurfaceObject_t surface, int x, int y, void* output)
	{
		char tmp = ::surf2Dread<char>(surface, x, y) ;
		((char*)output)[0] = tmp ;
	}
	#endif

	template <typename T>
	static inline __device__ void surf2Dread(void*, cudaSurfaceObject_t surface, int x, int y, T* output)
	{
		#if (__CUDA_ARCH__ >= 300)
		sizedSurf2Dread <sizeof(T)>(surface, x, y, output);
		#endif
	}

	#if (__CUDA_ARCH__ >= 300)
	template <int size> inline __device__ void sizedSurf2Dwrite (cudaSurfaceObject_t surface, int x, int y, void* output) ;
	template<> inline __device__ void sizedSurf2Dwrite<16>(cudaSurfaceObject_t surface, int x, int y, void* input)
	{
		int4 tmp ;
		tmp.x = ((int*)input)[0] ;
		tmp.y = ((int*)input)[1] ;
		tmp.z = ((int*)input)[2] ;
		tmp.w = ((int*)input)[3] ;
		::surf2Dwrite(tmp, surface, x * 16, y) ;
	}
	template<> inline __device__ void sizedSurf2Dwrite<8>(cudaSurfaceObject_t surface, int x, int y, void* input)
	{
		int2 tmp ;
		tmp.x = ((int*)input)[0] ;
		tmp.y = ((int*)input)[1] ;
		::surf2Dwrite(tmp, surface, x * 8, y) ;
	}
	template<> inline __device__ void sizedSurf2Dwrite<4>(cudaSurfaceObject_t surface, int x, int y, void* input)
	{
		int tmp ;
		tmp = ((int*)input)[0] ;
		::surf2Dwrite(tmp, surface, x * 4, y) ;
	}
	template<> inline __device__ void sizedSurf2Dwrite<2>(cudaSurfaceObject_t surface, int x, int y, void* input)
	{
		short tmp ;
		tmp = ((short*)input)[0] ;
		::surf2Dwrite(tmp, surface, x * 2, y) ;
	}
	template<> inline __device__ void sizedSurf2Dwrite<1>(cudaSurfaceObject_t surface, int x, int y, void* input)
	{
		char tmp ;
		tmp = ((char*)input)[0] ;
		::surf2Dwrite(tmp, surface, x, y) ;
	}
	#endif

	template <typename T>
	static inline __device__ void surf2Dwrite(void*, cudaSurfaceObject_t surface, int x, int y, T* input)
	{
		#if (__CUDA_ARCH__ >= 300)
		sizedSurf2Dwrite<sizeof(T)>(surface, x, y, input) ;
		#endif
	}

};
#endif
#pragma endregion

#ifndef ATOMICADD_PDD
#define ATOMICADD_PDD
#if __CUDACC_VER_MAJOR__ < 8
__device__ static inline double atomicAdd(double* address, double val) 
{ 
	unsigned long long int* address_as_ull = (unsigned long long int*)address; 
	unsigned long long int old = *address_as_ull, assumed; 
	do { 
		assumed = old; 
		old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));
		// Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN) 
	} while (assumed != old); return __longlong_as_double(old); 
}
#endif
#endif //ATOMICADD_PDD

#ifndef	ATOMICMAX_PDD
#define ATOMICMAX_PDD
__device__ static inline double atomicMax(double* address, double val)
{
    unsigned long long int * addr_as_ull = (unsigned long long int *)address;
    unsigned long long int old = *addr_as_ull;
    unsigned long long  assumed;

    do {
        assumed = old;

        if (val > __longlong_as_double(assumed))
            old = atomicCAS(addr_as_ull, assumed, __double_as_longlong(val));
        else
            break;
    } while(assumed != old);

    return __longlong_as_double(old);
}
#endif //ATOMICMAX_PDD


struct int8 {
	int x, y, z, w, x2,y2,z2,w2 ;

	hyb_device hyb_inline int8() {}
	hyb_device hyb_inline int8(int xx, int yy, int zz, int ww, int xx2, int yy2, int zz2, int ww2) {
		x = xx; y = yy; z = zz; w = ww;
		x2 = xx2; y2 = yy2; z2 = zz2; w2 = ww2;
	} 
};

hyb_inline hyb_device static int8 operator+(const int8& l, const int8&r) {
	int8 res;
	int* pres = (int*)&res;
	int* pl = (int*)&l;
	int* pr = (int*)&r;
	for(int i = 0; i < 8; ++i) {
		pres[i] = pl[i] + pr[i];
	}

	return res;
}

struct bool8 {
	unsigned char mask;
};

struct float8 { 

	typedef bool8 masktype ;

	float x, y, z, w, x2, y2, z2, w2;

	hyb_device hyb_inline float8() {}
	hyb_device hyb_inline float8(const float8& p) 
	{
		x  = p.x  ;
		y  = p.y  ;
		z  = p.z  ;
		w  = p.w  ;
		x2 = p.x2 ;
		y2 = p.y2 ;
		z2 = p.z2 ;
		w2 = p.w2 ;
	}
	hyb_device hyb_inline float8(float ix, float iy, float iz, float iw, float ix2, float iy2, float iz2, float iw2) 
	{
		x  = ix  ;
		y  = iy  ;
		z  = iz  ;
		w  = iw  ;
		x2 = ix2 ;
		y2 = iy2 ;
		z2 = iz2 ;
		w2 = iw2 ;
	}
};


hyb_device hyb_inline static float8 operator*(const float8& l, const float8&r) {
	float8 res;
	float* pres = (float*)&res;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		pres[i] = pl[i] * pr[i];
	}

	return res;
}

hyb_device hyb_inline static float8 operator*(const float8& l, const float&r) {
	float8 res;
	res.x = l.x * r;
	res.y = l.y * r;
	res.z = l.z * r;
	res.w = l.w * r;
	res.x2 = l.x2 * r;
	res.y2 = l.y2 * r;
	res.z2 = l.z2 * r;
	res.w2 = l.w2 * r;
	return res;
}

hyb_device hyb_inline static float8 operator/(const float8& l, const float8& r) {
	
	float8 res;
	float* pres = (float*)&res;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		pres[i] = pl[i] / pr[i];
	}

	return res;
}

hyb_device hyb_inline static float8 operator/(const float8& l, const float& r) {
	float8 res;
	res.x = l.x / r;
	res.y = l.y / r;
	res.z = l.z / r;
	res.w = l.w / r;
	res.x2 = l.x2 / r;
	res.y2 = l.y2 / r;
	res.z2 = l.z2 / r;
	res.w2 = l.w2 / r;
	return res;
}

hyb_device hyb_inline static float8 operator-(const float8& l, const float8& r) {
	
	float8 res;
	float* pres = (float*)&res;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		pres[i] = pl[i] - pr[i];
	}

	return res;
}


hyb_device hyb_inline static bool8 operator<(const float8& l, const float8& r) {
	
	bool8 res;
	res.mask = 0;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		res.mask |= ((pl[i] < pr[i]) << i);
	}

	return res;
}

hyb_device hyb_inline static bool8 operator>(const float8& l, const float8& r) {
	
	bool8 res;
	res.mask = 0;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		res.mask |= ((pl[i] > pr[i]) << i);
	}

	return res;
}

hyb_device hyb_inline static bool8 operator>(const float8& l, const float& r) {
	
	bool8 res;
	res.mask = 0;
	float* pl = (float*)&l;
	#pragma unroll
	for(int i = 0; i < 8; ++i) {
		res.mask |= ((pl[i] > r) << i);
	}

	return res;
}

hyb_device hyb_inline static float8 operator+(const float8& l, const float8&r) {
	float8 res;
	float* pres = (float*)&res;
	float* pl = (float*)&l;
	float* pr = (float*)&r;
	for(int i = 0; i < 8; ++i) {
		pres[i] = pl[i] + pr[i];
	}

	return res;
}

hyb_device hyb_inline static float8 operator+(const float8& l, const float&r) {
	float8 res;
	res.x = l.x + r;
	res.y = l.y + r;
	res.z = l.z + r;
	res.w = l.w + r;
	res.x2 = l.x2 + r;
	res.y2 = l.y2 + r;
	res.z2 = l.z2 + r;
	res.w2 = l.w2 + r;
	return res;
}

namespace hybridizer {
	template<typename Vec, typename Mask>
	hyb_device hyb_inline Vec select(const Mask& m, const Vec& l, const Vec& r);

	template<>
	hyb_device hyb_inline float8 select<float8, bool8>(const bool8& m, const float8& l, const float8& r) {
		float8 res;
		float* pres = (float*)&res;
		float* pl = (float*)&l;
		float* pr = (float*)&r;
		int mask = 1;
		for(int i = 0; i < 8; ++i) {
			if(m.mask & mask) {
				pres[i] = pl[i];
			}
			else {
				pres[i] = pr[i];
			}

			mask <<= 1;
		}
		return res;
	}

	template<typename Vec, typename Scal>
	hyb_device  hyb_inline Vec insertElement(const Vec& a, Scal elem, int index);

	template<>
	hyb_device  hyb_inline int8 insertElement(const int8& vec, int elem, int index) {
		int8 result(vec);
		int* pr = (int*)&result;
		int* pv = (int*)&vec;
		pr[index] = pv[index];
		return result;
	}
	
	template<>
	hyb_device  hyb_inline float8 insertElement(const float8& vec, float elem, int index) {
		float8 result(vec);
		float* pr = (float*)&result;
		float* pv = (float*)&vec;
		pr[index] = pv[index];

		return result;
	}
	
	template<>
	hyb_device  hyb_inline float4 insertElement(const float4& vec, float elem, int index) {
		float4 result(vec);
		float* pr = (float*)&result;
		float* pv = (float*)&vec;
		pr[index] = pv[index];

		return result;
	}

	template<typename Vec, typename Scal>
	hyb_device  hyb_inline Scal extractElement(const Vec& vec, int index);

	template<>
	hyb_device hyb_inline int extractElement(const int8& vec, int index) {
		int* pv = (int*) &vec;
		return pv[index];
	}

	template<>
	hyb_device hyb_inline float extractElement(const float8& vec, int index) {
		float* pv = (float*) &vec;
		return pv[index];
	}

	template<>
	hyb_device hyb_inline float extractElement(const float4& vec, int index) {
		float* pv = (float*) &vec;
		return pv[index];
	}

	template<typename VecOutput, typename VecInput, typename Mask>
	hyb_device hyb_inline VecOutput shuffleVector(const VecInput& l, const VecInput& r, const Mask& m);

	template<>
	hyb_device hyb_inline int8 shuffleVector(const int8& l, const int8& r, const int8& m) {
		int8 res;
		int* pl = (int*)&l;
		int* pr = (int*)&r;
		int* pm = (int*)&m;
		int* pres = (int*)&res;
		#pragma unroll
		for(int i = 0; i < 8; ++i) {
			int index = pm[i];
			if(index >= 0 && index < 8) 
				pres[i] = pl[index];
			else if(index >= 0) 
				pres[i] = pr[index];
		}
		return res;
	}

	template<>
	hyb_device hyb_inline float8 shuffleVector(const float8& l, const float8& r, const int8& m) {
		float8 res;
		float* pl = (float*)&l;
		float* pr = (float*)&r;
		int* pm = (int*)&m;
		float* pres = (float*)&res;
		#pragma unroll
		for(int i = 0; i < 8; ++i) {
			int index = pm[i];
			if(index >= 0 && index < 8) 
				pres[i] = pl[index];
			else if(index >= 0) 
				pres[i] = pr[index];
		}

		return res;
	}

}

static inline  __device__ float4 hybrid_ldg(float4* x) { return *x ; }
static inline  __device__ double hybrid_ldg(double* x) { return *x ; }

__device__ static inline float4 operator+(float4 a, float b)
{
	float4 res ;
	res.x = a.x + b ;
	res.y = a.y + b ;
	res.z = a.z + b ;
	res.w = a.w + b ;
	return res ;
}


__device__ static inline float4 operator*(float4 a, float b)
{
	float4 res ;
	res.x = a.x * b ;
	res.y = a.y * b ;
	res.z = a.z * b ;
	res.w = a.w * b ;
	return res ;
}

__device__ static inline float4 operator+(float a, float4 b)
{
	float4 res ;
	res.x = a + b.x ;
	res.y = a + b.y ;
	res.z = a + b.z ;
	res.w = a + b.w ;
	return res ;
}


__device__ static inline float4 operator*(float a, float4 b)
{
	float4 res ;
	res.x = a * b.x ;
	res.y = a * b.y ;
	res.z = a * b.z ;
	res.w = a * b.w ;
	return res ;
}

__device__ static inline float4 operator+(float4 a, float4 b)
{
	float4 res ;
	res.x = a.x + b.x ;
	res.y = a.y + b.y ;
	res.z = a.z + b.z ;
	res.w = a.w + b.w ;
	return res ;
}


__device__ static inline float4 operator*(float4 a, float4 b)
{
	float4 res ;
	res.x = a.x * b.x ;
	res.y = a.y * b.y ;
	res.z = a.z * b.z ;
	res.w = a.w * b.w ;
	return res ;
}

/*
#if __CUDA_ARCH__ < 35
__device__ float4 hybrid_ldg(float4* x) { return *x ; }
#else
extern "C" __device__ float4 _ldg(float4*);
#define hybrid_ldg __ldg
#endif
*/

namespace hybridizer {
	template <typename F> struct hybdelegate {
		union {hybridobject* instance; char padding_instance[8]; };
		union {F functor; char padding_functor[8]; };

		// compare to null, and assign to null
		hyb_device hyb_inline hybdelegate<F>& operator= (F func)
		{
			functor = func ;
			return *this ;
		}
		hyb_device hyb_inline bool operator== (F func)
		{
			return func == functor ;
		}
	};

	struct hybdatetime {	
		uint64_t ticks;
		hyb_device hyb_inline int64_t get_Ticks() { return (int64_t) (ticks & 0x3fffffffffffffff); }
		hyb_device hyb_inline bool operator==(hybdatetime t) { return t.ticks == ticks; }
	};
}

#pragma region pointer arithmetics

namespace hybridizer {
	template <typename T>
	struct nativearrayindexer
	{
		T* _ptr ;
		hyb_device hyb_inline static nativearrayindexer<T> build (void* ptr, size_t index)
		{
			nativearrayindexer<T> res ;
			res._ptr = (T*) (((char*)ptr) + index) ;
			return res ;
		}

		hyb_device hyb_inline static void store (const nativearrayindexer<T>* ptr, const T& value)
		{
			*(ptr->_ptr) = value ;
		}
		

		hyb_device hyb_inline static T* getpointer (const nativearrayindexer<T>& ptr)
		{
			return ptr._ptr;
		}

		template <typename U>
		hyb_device hyb_inline static U load (const nativearrayindexer<T>* ptr)
		{
			return (U)(*(ptr->_ptr));
		}
	};

	// in the special case of T = void* (which happens when code is generated from llvm), we just want to be able to
	// - build
	// - get the pointer and 
	// - cast to another type. 
	// load and store would have no meaning, so implementation is not provided
	template <>
	struct nativearrayindexer< void > {
		void* _ptr;

		hyb_device hyb_inline static nativearrayindexer<void> build (void* ptr, size_t index)
		{
			nativearrayindexer<void> res ;
			res._ptr = (void*) (((char*)ptr) + index) ;
			return res ;
		}

		hyb_device hyb_inline static void* getpointer (const nativearrayindexer<void*>& ptr)
		{
			return ptr._ptr;
		}

		template <typename U>
		hyb_device hyb_inline operator nativearrayindexer<U>() 
		{
			return *(nativearrayindexer<U>*)(void*)this;
		}
	};
}

#pragma endregion

#pragma region fixed buffers

namespace hybridizer
{
	template<typename T, int count>
	struct fixedbuffer 
	{
		union
		{
			T FixedElementField ;
			T __data [count] ;
		} ;
	} ;
}


#pragma endregion

#pragma region llvm memset/memcpy
namespace hybridizer {
	template<int align> // alignment in bytes
	hyb_device hyb_inline void memseti32(char* ptr, char val, int size) ;

	template<>
	hyb_device hyb_inline void memseti32<32>(char* ptr, char val, int size)
	{
		if (val == 0)
		{
			int4 ival; ival.x = ival.y = ival.z = ival.w = 0 ;
			int4* aptr = (int4*)ptr ;
			#pragma unroll
			for (int i = 0 ; i < size / 16 ; ++i)
			{
				aptr[i] = ival ;
			}
		} else {
			int iival = val | (((int)val) << 8) | (((int)val) << 16)| (((int)val) << 24);
			int4 ival; ival.x = ival.y = ival.z = ival.w = iival ;
			int4* aptr = (int4*)ptr ;
			#pragma unroll
			for (int i = 0 ; i < size / 16 ; ++i)
			{
				aptr[i] = ival ;
			}
		}
	}

	template<int align> // alignment in bytes
	hyb_device hyb_inline void memseti32(char* ptr, char val, int size) 
	{
		memset(ptr, val, size);
	}

	template<int align>
	hyb_device hyb_inline void memcpyi32(char* ptr, char* src, int size) ;
	
	template<>
	hyb_device hyb_inline void memcpyi32<32>(char* dest, char* src, int size)
	{
		int4* adest = (int4*)dest ;
		int4* asrc = (int4*)src ;
		#pragma unroll
		for (int i = 0 ; i < size / 16 ; ++i)
		{
			adest[i] = asrc[i] ;
		}
	}
	
	template<>
	hyb_device hyb_inline void memcpyi32<4>(char* dest, char* src, int size)
	{
		int* adest = (int*)dest ;
		int* asrc = (int*)src ;
		#pragma unroll
		for (int i = 0 ; i < size / 4 ; ++i)
		{
			adest[i] = asrc[i] ;
		}
	}
	
	template<int align>
	hyb_device hyb_inline void memcpyi32(char* ptr, char* src, int size)
	{
		memcpy(ptr, src, size);
	}

	template<int align>
	hyb_device hyb_inline void memcpyi64(char* ptr, char* src, int64_t size) ;
	
	template<>
	hyb_device hyb_inline void memcpyi64<32>(char* dest, char* src, int64_t size)
	{
		int4* adest = (int4*)dest ;
		int4* asrc = (int4*)src ;
		#pragma unroll
		for (int64_t i = 0 ; i < size / 16L ; ++i)
		{
			adest[i] = asrc[i] ;
		}
	}
	
	template<>
	hyb_device hyb_inline void memcpyi64<4>(char* dest, char* src, int64_t size)
	{
		int* adest = (int*)dest ;
		int* asrc = (int*)src ;
		#pragma unroll
		for (int64_t i = 0 ; i < size / 4L ; ++i)
		{
			adest[i] = asrc[i] ;
		}
	}
	
	template<int align>
	hyb_device hyb_inline void memcpyi64(char* ptr, char* src, int64_t size)
	{
		memcpy(ptr, src, size);
	}
}

#define __hybridizer_memseti32(ptr,val,count,align,isvolatile) hybridizer::memseti32<align>(ptr,val,count)
#define __hybridizer_memcpyi32(dest,src,len,align,isvolatile) hybridizer::memcpyi32<align>(dest,src,len)
#define __hybridizer_memcpyi64(dest,src,len,align,isvolatile) hybridizer::memcpyi64<align>(dest,src,len)
#pragma endregion

#pragma region vector load/store
namespace hybridizer {
	template<int alignment>
	hyb_inline hyb_device float8 loadfloat8(const float8* ptr);

	template<>
	hyb_inline hyb_device float8 loadfloat8<32>(const float8* ptr) {
		float8 res;
		float4 low = ((float4*)(ptr))[0];
		float4 high = ((float4*)(ptr))[1];
		res.x = low.x;
		res.y = low.y;
		res.z = low.z;
		res.w = low.w;
		res.x2 = high.x;
		res.y2 = high.y;
		res.z2 = high.z;
		res.w2 = high.w;
		return res;
	}
	
	template<int alignment>
	hyb_inline hyb_device float8 loadfloat8(const float8* ptr) {
		return *ptr;
	}

	template<int alignment>
	hyb_inline hyb_device void storefloat8(float8* ptr, const float8& val);

	template<>
	hyb_inline hyb_device void storefloat8<32>(float8* ptr, const float8& val) {
		float4* iptr = (float4*) ptr;
		float4* ival = (float4*) &val;
		iptr[0] = ival[0];
		iptr[1] = ival[1];
	}
	
	template<int alignment>
	hyb_inline hyb_device void storefloat8(float8* ptr, const float8& val) {
		*ptr = val;
	}

#define __hybridizer_load_float8(ptr, alignment) hybridizer::loadfloat8<alignment>(ptr)
#define __hybridizer_store_float8(ptr, val, alignment) hybridizer::storefloat8<alignment>(ptr, val)
}

#pragma endregion

#pragma region actions
// #ifndef HYBRIDIZER_NO_HOST
namespace hybridizer {
	// TODO?: something better than a function pointer but what??
	
#ifdef __cpp_variadic_templates
	template<typename ...T>
	struct action {
		typedef void(*funcaction)(void* self, T... i);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr; char padding2[8]; };
		union { void* _funcptrvect; char padding3[8]; };

		hyb_device hyb_inline action() {}
		hyb_device hyb_inline action(void* self, void* func) : _self(self), _funcptr((funcaction)func) {}

		hyb_device hyb_inline void invoke(void* self, T... i) {
			_funcptr(self, i...); 
		}

		hyb_device hyb_inline void invoke(T... i) {
			_funcptr(_self, i...);
		}
	};

	/// action2, action3 and so on are useless in cuda (since we have no vectorization issue). We just need action
	template<typename ...T>
	using action2 = action<T...>;
	template<typename ...T>
	using action3 = action<T...>;
	template<typename ...T>
	using action4 = action<T...>;
#else // visual < 2015
	template<typename T>
	struct action {
		typedef void(*funcaction)(void* self, T i);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr; char padding2[8]; };
		union { void* _funcptrvect; char padding3[8]; };

		hyb_device hyb_inline action() {}
		hyb_device hyb_inline action(void* self, void* func) : _self(self), _funcptr((funcaction)func) {}

		hyb_device hyb_inline void invoke(void* self, T i) {
			_funcptr(self, i); 
		}

		hyb_device hyb_inline void invoke(T i) {
			_funcptr(_self, i);
		}
	};

	template<typename T1, typename T2>
	struct action2 {
		typedef void(*funcaction)(void* self, T1 i1, T2 i2);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr; char padding2[8]; };
		union { void* _funcptrvect; char padding3[8]; };

		hyb_device hyb_inline action2() {}
		hyb_device hyb_inline action2(void* self, void* func) : _self(self), _funcptr((funcaction)func) {}

		hyb_device hyb_inline void invoke(void* self, T1 i1, T2 i2) {
			_funcptr(self, i1, i2); 
		}

		hyb_device hyb_inline void invoke(T1 i1, T2 i2) {
			_funcptr(_self, i1, i2);
		}
	};

	template<typename T1, typename T2, typename T3>
	struct action3 {
		typedef void(*funcaction)(void* self, T1 i1, T2 i2, T3 i3);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr; char padding2[8]; };
		union { void* _funcptrvect; char padding3[8]; };

		hyb_device hyb_inline action3() {}
		hyb_device hyb_inline action3(void* self, void* func) : _self(self), _funcptr((funcaction)func) {}

		hyb_device hyb_inline void invoke(void* self, T1 i1, T2 i2, T3 i3) {
			_funcptr(self, i1, i2, i3); 
		}

		hyb_device hyb_inline void invoke(T1 i1, T2 i2, T3 i3) {
			_funcptr(_self, i1, i2, i3);
		}
	};

	template<typename T1, typename T2, typename T3, typename T4>
	struct action4 {
		typedef void(*funcaction)(void* self, T1 i1, T2 i2, T3 i3, T4 i4);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr; char padding2[8]; };
		union { void* _funcptrvect; char padding3[8]; };

		hyb_device hyb_inline action4() {}
		hyb_device hyb_inline action4(void* self, void* func) : _self(self), _funcptr((funcaction)func) {}

		hyb_device hyb_inline void invoke(void* self, T1 i1, T2 i2, T3 i3, T4 i4) {
			_funcptr(self, i1, i2, i3, i4); 
		}

		hyb_device hyb_inline void invoke(T1 i1, T2 i2, T3 i3, T4 i4) {
			_funcptr(_self, i1, i2, i3, i4);
		}
	};
#endif

	// static actions
	#ifndef HYBRIDIZER_NO_HOST
	template<typename T, hyb_device void (*func)(T)>
	#else
	template<typename T, void (*func)(T)>
	#endif
	struct action_static
	{
		hyb_device hyb_inline operator action<T> () const { return action<T>(NULL, invoke_ptr) ; } // nullptr not supported by nvcc <dummy>
		hyb_device hyb_inline void invoke (T t) { return func (t) ; }
		hyb_device hyb_inline void invoke (void* self, T t) { return func (t) ; }
		hyb_device hyb_inline static void invoke_ptr (void* self, T t) { return func (t) ; }
	};

	template <typename T>
	hyb_device hyb_inline void parallelfor(void* self, int start, int stop, action<T>* action);

	template <>
	hyb_device hyb_inline void parallelfor<int>(void* self, int start, int stop, action<int>* action)
	{
		for(int i = __hybridizer_threadIdxX + __hybridizer_blockIdxX * __hybridizer_blockDimX + start; i < stop; i += __hybridizer_blockDimX * __hybridizer_gridDimX) {
			action->invoke(self, i);
		}
	}

	template <typename T1, typename T2>
	hyb_device hyb_inline void parallelfor2D(void* self, int startX, int stopX, int startY, int stopY, action2<T1, T2>* action);

	template<>
	hyb_device hyb_inline void parallelfor2D(void* self, int startX, int stopX, int startY, int stopY, action2<int, int>* action) {
		for (int i = __hybridizer_threadIdxY + __hybridizer_blockIdxY * __hybridizer_blockDimY + startY; i < stopY; i += __hybridizer_blockDimY * __hybridizer_gridDimY) {
			for (int j = __hybridizer_threadIdxX + __hybridizer_blockIdxX * __hybridizer_blockDimX + startX; j < stopX; j += __hybridizer_blockDimX * __hybridizer_gridDimX) {
				action->invoke(self, i, j);
			}
		}
	}

	template <typename T>
	hyb_device hyb_inline void parallelfor(int start, int stop, action<T> action);

	template <>
	hyb_device hyb_inline void parallelfor<int>(int start, int stop, action<int> action)
	{
		for(int i = __hybridizer_threadIdxX + __hybridizer_blockIdxX * __hybridizer_blockDimX + start; i < stop; i += __hybridizer_blockDimX * __hybridizer_gridDimX) {
			action.invoke(i);
		}
	}

	template <typename T1, typename T2>
	hyb_device hyb_inline void parallelfor2D(int startX, int stopX, int startY, int stopY, action2<T1, T2> action);

	template <>
	hyb_device hyb_inline void parallelfor2D<int>(int startX, int stopX, int startY, int stopY, action2<int, int> action)
	{
		for(int i = __hybridizer_threadIdxY + __hybridizer_blockIdxY * __hybridizer_blockDimY + startY; i < stopY; i += __hybridizer_blockDimY * __hybridizer_gridDimY) {
			for(int j = __hybridizer_threadIdxX + __hybridizer_blockIdxX * __hybridizer_blockDimX + startX; j < stopX; j += __hybridizer_blockDimX * __hybridizer_gridDimX) {
				action.invoke(i, j);
			}
		}
	}
}

// #endif
#pragma endregion

#pragma region Func<...>
#ifndef HYBRIDIZER_NO_HOST

namespace hybridizer 
{
	template<typename T, typename U, typename V, typename W, typename retV>
	struct func4
	{
		typedef retV (*funcaction)(void* self, T i, U j, V k, W l);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr ; char padding2[8]; };
		union { void* _funcptrvect ; char padding3[8]; };

		hyb_device hyb_inline func4 () {}
		hyb_device hyb_inline func4 (void* self, void* func) : _self(self), _funcptr ((funcaction)func) { }

		hyb_device hyb_inline retV invoke(void* self, T i, U j, V k, W l) const {
			return _funcptr(self, i, j, k, l);
		}

		hyb_device hyb_inline retV invoke(T i, U j, V k, W l) const {
			return _funcptr(_self, i, j, k, l);
		}
	} ;

	template<typename T, typename U, typename V, typename W, typename retV, hyb_device retV (*funcptr)(T,U,V)>
	struct func4_static
	{
		// http://www.cplusplus.com/forum/beginner/11866/
		hyb_device hyb_inline operator func4<T,U,V,W,retV> () const { return func4<T,U,V,W,retV>(NULL, invoke_ptr) ; }
		hyb_device hyb_inline retV invoke (T t, U u, V v, W w) { return funcptr (t,u,v, w) ; }
		hyb_device hyb_inline retV invoke (void* self, T t, U u, V v, W w) { return funcptr (t,u,v,w) ; }
		hyb_device hyb_inline static retV invoke_ptr (void* self, T t, U u, V v, W w) { return funcptr (t,u,v,w) ; }
	} ;

	template<typename T, typename U, typename V, typename retV>
	struct func3
	{
		typedef retV (*funcaction)(void* self, T i, U j, V k);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr ; char padding2[8]; };
		union { void* _funcptrvect ; char padding3[8]; };

		hyb_device hyb_inline func3 () {}
		hyb_device hyb_inline func3 (void* self, void* func) : _self(self), _funcptr ((funcaction)func) { }

		hyb_device hyb_inline retV invoke(void* self, T i, U j, V k) {
			return _funcptr(self, i, j, k);
		}

		hyb_device hyb_inline retV invoke(T i, U j, V k) {
			return _funcptr(_self, i, j, k);
		}
	} ;

	template<typename T, typename U, typename V, typename retV, hyb_device retV (*funcptr)(T,U,V)>
	struct func3_static
	{
		// http://www.cplusplus.com/forum/beginner/11866/
		hyb_device hyb_inline operator func3<T,U,V,retV> () const { return func3<T,U,V,retV>(NULL, invoke_ptr) ; }
		hyb_device hyb_inline retV invoke (T t, U u, V v) { return funcptr (t,u,v) ; }
		hyb_device hyb_inline retV invoke (void* self, T t, U u, V v) { return funcptr (t,u,v) ; }
		hyb_device hyb_inline static retV invoke_ptr (void* self, T t, U u, V v) { return funcptr (t,u,v) ; }
	} ;

	template<typename T, typename U, typename retV>
	struct func2
	{
		typedef retV (*funcaction)(void* self, T i, U j);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr ; char padding2[8]; };
		union { void* _funcptrvect ; char padding3[8]; };

		hyb_device hyb_inline func2 () {}
		hyb_device hyb_inline func2 (void* self, void* func) : _self(self), _funcptr ((funcaction)func) { }

		hyb_device hyb_inline retV invoke(void* self, T i, U j) {
			return _funcptr(self, i, j);
		}

		hyb_device hyb_inline retV invoke(T i, U j) {
			return _funcptr(_self, i, j);
		}
	} ;

	template<typename T, typename U, typename retV, hyb_device retV (*funcptr)(T,U)>
	struct func2_static
	{
		// http://www.cplusplus.com/forum/beginner/11866/
		hyb_device hyb_inline operator func2<T,U,retV> () const { return func2<T,U,retV>(NULL, invoke_ptr) ; }
		hyb_device hyb_inline retV invoke (T t, U u) { return funcptr (t,u) ; }
		hyb_device hyb_inline retV invoke (void* self, T t, U u) { return funcptr (t,u) ; }
		hyb_device hyb_inline static retV invoke_ptr (void* self, T t, U u) { return funcptr (t,u) ; }
	} ;


	template<typename S, typename T, typename U, typename retV, hyb_device retV(*funcptr)(S*, T, U)>
	struct func2_capture_static
	{
		hyb_device hyb_inline operator func2<T, U, retV>() const { return func2<T, U, retV>(NULL, invoke_ptr); }
		hyb_device hyb_inline retV invoke(T t, U u) { return funcptr(NULL, t, u); }
		hyb_device hyb_inline retV invoke(S* self, T t, U u) { return funcptr(self, t, u); }
		hyb_device hyb_inline static retV invoke_ptr(S* self, T t, U u) { return funcptr(self, t, u); }
	};

	template<typename T, typename retV>
	struct func1
	{
		typedef retV (*funcaction)(void* self, T i);

		union { void* _self; char padding1[8]; };
		union { funcaction _funcptr ; char padding2[8]; };
		union { void* _funcptrvect ; char padding3[8]; };

		static hyb_device hyb_inline func1<T, retV> null_ptr() {
			func1 res;
			return res;
		}

		hyb_device hyb_inline operator void*() { return _self; }

		hyb_device hyb_inline func1 () {}
		hyb_device hyb_inline func1 (void* self, void* func) : _self(self), _funcptr ((funcaction)func) { }

		hyb_device hyb_inline retV invoke(void* self, T i) {
			return _funcptr(self, i);
		}

		hyb_device hyb_inline retV invoke(T i) {
			return _funcptr(_self, i);
		}
	};

	template<typename T, typename retV, hyb_device retV (*funcptr)(T)>
	struct func1_static
	{
		// http://www.cplusplus.com/forum/beginner/11866/
		hyb_device hyb_inline operator func1<T,retV> () const { return func1<T,retV>(NULL, invoke_ptr) ; }
		hyb_device hyb_inline retV invoke (T t) { return funcptr (t) ; }
		hyb_device hyb_inline retV invoke (void* self, T t) { return funcptr (t) ; }
		hyb_device hyb_inline static retV invoke_ptr (void* self, T t) { return funcptr (t) ; }
	} ;
}

#endif
#pragma endregion

namespace hybridizer {
	template <typename T> struct nullable {
		T data;
		union {
			bool hasValue;
			char __padding[(sizeof(T)  - 4) % 4 + 4];
		};

		hyb_device hyb_inline bool get_HasValue() { return hasValue; }
		hyb_device hyb_inline T get_Value() { return data; }
	};

	struct string {
		const char* data;

		#if !defined(_WIN32)
		hyb_inline hyb_device string() {} 
		hyb_inline hyb_device string(const char* p) : data(p) {}
		#endif

		#if !defined(HYBRIDIZER_NO_HOST)
		friend hyb_inline hyb_device int operator==(const hybridizer::string& s1, const hybridizer::string& s2) {
			return s1.data == s2.data;
		}
		#endif

		hyb_inline hyb_device operator const char*() {return data;}
	};

	
	#if !defined(HYBRIDIZER_NO_HOST)
	hyb_device hyb_inline static void hyprintfline(FILE* f, const string& message) {
		printf("%s\n", message.data);
	}
	#else
	hyb_device hyb_inline static void hyprintfline(void* f, const string& message) {
		printf("%s\n", message.data);
	}
	#endif
}

namespace hybridizer
{
	template <typename T>
	struct alignedindex 
	{
		T inner ;

		static hyb_device hyb_inline alignedindex <T> op_Implicit (T t) { alignedindex <T> res ; res.inner = t ; return res ; }
		hyb_device hyb_inline T get_Inner () { return inner ; } 

		// hyb_device hyb_inline alignedindex<T> operator +(T rht) const { alignedindex<T> res ; res.inner = inner + rht ; return res ; }

		hyb_device hyb_inline alignedindex<T> () {}
		hyb_device hyb_inline alignedindex<T> (T i) { inner = i ; }
		hyb_device hyb_inline operator int () const { return inner ; }

		hyb_device hyb_inline bool operator<(T i) const { return inner < i ; }
		hyb_device hyb_inline bool operator>(T i) const { return inner > i ; }
		hyb_device hyb_inline bool operator<=(T i) const { return inner <= i ; }
		hyb_device hyb_inline bool operator>=(T i) const { return inner >= i ; }
		hyb_device hyb_inline bool operator==(T i) const { return inner == i ; }
		hyb_device hyb_inline bool operator!=(T i) const { return inner != i ; }
	} ;

	template <typename T>
	struct alignedstorage
	{
		union
		{
			T* inner ;
			char __padding[8];
		};
		
		hyb_inline hyb_device T get_Item (const int & index) const
		{
			return inner[index];
		}

		hyb_inline hyb_device  void set_Item (int index, T value) { 
			inner[index] = value; 
		}
	} ;

	template <typename T>
	struct stackarray
	{
		union
		{
			T* inner;
			char __padding[8];
		};
		
		hyb_inline hyb_device T get_Item (const int & index) const
		{
			return inner[index];
		}

		hyb_inline hyb_device  void set_Item (int index, const T& value) { 
			inner[index] = value; 
		}
	} ;
}

namespace hybridizer
{
	template<typename T>
	hyb_inline hyb_device void swap (T* a, T* b)
	{
		T p = *b ; *b = *a ; *a = p ;
	}
}


namespace hybridizer {
	template<typename From, typename To>
	hyb_inline hyb_device To bitcast(const From& f) 
	{
		return *((To*)&f);
	}
}

#ifdef HYB_CUDA_HALF
#include <hybridizer.cuda.half.cuh>
#endif

namespace hybridizer {
	hyb_inline hyb_device bool enforce_serial() 
	{
		return threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0;
	}
}

// ----- /HYBRIDIZER_CUDA_CUH ----- 
#include <cstdio>
// generating GetTypeID function
#include <cstring> // for strcmp
extern "C" DLL_PUBLIC int HybridizerGetTypeID( const char* fullTypeName)
{
	if (strcmp (fullTypeName, "Hybrid.Program") == 0) return 1000000 ; 
	if (strcmp (fullTypeName, "Hybridizer.Runtime.CUDAImports.IntResidentArray") == 0) return 1000001 ; 
	if (strcmp (fullTypeName, "Hybridizer.Runtime.CUDAImports.ResidentArrayStatus") == 0) return 1000002 ; 
	if (strcmp (fullTypeName, "System.Action<System.Int32>") == 0) return 1000003 ; 
	if (strcmp (fullTypeName, "System.Nullable<System.Int64>") == 0) return 1000004 ; 
	if (strcmp (fullTypeName, "System.Object") == 0) return 1000005 ; 
	if (strcmp (fullTypeName, "System.Threading.Tasks.Parallel") == 0) return 1000006 ; 
	if (strcmp (fullTypeName, "System.Threading.Tasks.ParallelLoopResult") == 0) return 1000007 ; 
	return 0 ;
}
extern "C" DLL_PUBLIC const char* HybridizerGetTypeFromID( const int typeId)
{
	if (typeId == 1000000) return "Hybrid.Program" ; 
	if (typeId == 1000001) return "Hybridizer.Runtime.CUDAImports.IntResidentArray" ; 
	if (typeId == 1000002) return "Hybridizer.Runtime.CUDAImports.ResidentArrayStatus" ; 
	if (typeId == 1000003) return "System.Action<System.Int32>" ; 
	if (typeId == 1000004) return "System.Nullable<System.Int64>" ; 
	if (typeId == 1000005) return "System.Object" ; 
	if (typeId == 1000006) return "System.Threading.Tasks.Parallel" ; 
	if (typeId == 1000007) return "System.Threading.Tasks.ParallelLoopResult" ; 
	return "" ;
}
extern "C" DLL_PUBLIC int HybridizerGetShallowSize (const char* fullTypeName) 
{
	#ifdef __TYPE_DECL__Hybrid_Program___
	if (strcmp (fullTypeName, "Hybrid.Program") == 0) return 8 ; 
	#endif
	#ifdef __TYPE_DECL__Hybridizer_Runtime_CUDAImports_IntResidentArray___
	if (strcmp (fullTypeName, "Hybridizer.Runtime.CUDAImports.IntResidentArray") == 0) return 32 ; 
	#endif
	#ifdef __TYPE_DECL_int__
	if (strcmp (fullTypeName, "Hybridizer.Runtime.CUDAImports.ResidentArrayStatus") == 0) return 4 ; 
	#endif
	#ifdef __TYPE_DECL_hybridizer_action__T____
	if (strcmp (fullTypeName, "System.Action<System.Int32>") == 0) return 16 ; 
	#endif
	#ifdef __TYPE_DECL_hybridizer_nullable__T____
	if (strcmp (fullTypeName, "System.Nullable<System.Int64>") == 0) return 16 ; 
	#endif
	#ifdef __TYPE_DECL_hybridizer_hybridobject___
	if (strcmp (fullTypeName, "System.Object") == 0) return 8 ; 
	#endif
	#ifdef __TYPE_DECL__System_Threading_Tasks_ParallelLoopResult__
	if (strcmp (fullTypeName, "System.Threading.Tasks.ParallelLoopResult") == 0) return 24 ; 
	#endif
	return 0 ;
}

// Get various Hybridizer properties at runtime
struct __hybridizer_properties {
    int32_t UseHybridArrays;
    int32_t Flavor;
    int32_t CompatibilityMode;
    int32_t _dummy;
};
extern "C" DLL_PUBLIC __hybridizer_properties __HybridizerGetProperties () {
    __hybridizer_properties res;
    res.UseHybridArrays = 0;
    res.Flavor = 1;
    res.CompatibilityMode = 0;
    return res ;
}
#include <cuda.h>                                     
 struct HybridModule                                  
 {                                                    
     void* module_data ;                              
     CUmodule module ;                                
 } ;                                                  
                                                      
 extern char __hybridizer_cubin_module_data [] ;      
 static HybridModule __hybridizer__gs_module = { 0 }; 

#pragma region Wrappers definitions


extern "C" DLL_PUBLIC int Hybridx46Programx46Create_pre_basis_ExternCWrapper_CUDA( int gridDim_x,  int gridDim_y,  int blockDim_x,  int blockDim_y,  int blockDim_z,  int shared,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const equation,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const pre_basis_main,  int N)
{
	CUresult cures ;                                                                                 
	if (__hybridizer__gs_module.module_data == 0)                                                    
	{                                                                                              
		cures = cuModuleLoadData (&(__hybridizer__gs_module.module), __hybridizer_cubin_module_data) ; 
		if (cures != CUDA_SUCCESS) return (int)cures ;                                                 
	}                                                                                              
	                                                                                                 
	CUfunction __hybridizer__cufunc ;                                                                
	                                                                                                 
	cures = cuModuleGetFunction (&__hybridizer__cufunc, __hybridizer__gs_module.module, "Hybridx46Programx46Create_pre_basis") ;   
	if (cures != CUDA_SUCCESS) return (int)cures ;                                                   
	                                                                                                 
	hybridizer::runtime* __hybridizer_runtime = (hybridizer::runtime*) __hybridizer_init_basic_runtime(); 



	void* __hybridizer_launch_config[5] = 
		{
			(void*)&__hybridizer_runtime,
			(void*)&equation,
			(void*)&pre_basis_main,
			(void*)&N,
			(void*)0
		} ;

	shared += 16 ; if (shared > 48*1024) shared = 48*1024 ;                                                                                                
	                                                                                                                                                       
	cures = cuLaunchKernel (__hybridizer__cufunc, gridDim_x, gridDim_y, 1, blockDim_x, blockDim_y, blockDim_z, shared, 0, __hybridizer_launch_config, 0) ; 
	if (cures != CUDA_SUCCESS) return (int)cures ; 
	int cudaLaunchRes = (int)::cudaPeekAtLastError ();                                                                                                     
	if (cudaLaunchRes != 0) return cudaLaunchRes;                                                                                                          
	int __synchronizeRes = (int)::cudaDeviceSynchronize () ;                                                                                               
	return __synchronizeRes ;                                                                                                                              

}

extern "C" DLL_PUBLIC int Hybridx46Programx46Substitute_ExternCWrapper_CUDA( int gridDim_x,  int gridDim_y,  int blockDim_x,  int blockDim_y,  int blockDim_z,  int shared,  int* const equation,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const pre_basis,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const result,  int equationLength,  int pre_basisLengthAxis0)
{
	CUresult cures ;                                                                                 
	if (__hybridizer__gs_module.module_data == 0)                                                    
	{                                                                                              
		cures = cuModuleLoadData (&(__hybridizer__gs_module.module), __hybridizer_cubin_module_data) ; 
		if (cures != CUDA_SUCCESS) return (int)cures ;                                                 
	}                                                                                              
	                                                                                                 
	CUfunction __hybridizer__cufunc ;                                                                
	                                                                                                 
	cures = cuModuleGetFunction (&__hybridizer__cufunc, __hybridizer__gs_module.module, "Hybridx46Programx46Substitute") ;   
	if (cures != CUDA_SUCCESS) return (int)cures ;                                                   
	                                                                                                 
	hybridizer::runtime* __hybridizer_runtime = (hybridizer::runtime*) __hybridizer_init_basic_runtime(); 



	void* __hybridizer_launch_config[7] = 
		{
			(void*)&__hybridizer_runtime,
			(void*)&equation,
			(void*)&pre_basis,
			(void*)&result,
			(void*)&equationLength,
			(void*)&pre_basisLengthAxis0,
			(void*)0
		} ;

	shared += 16 ; if (shared > 48*1024) shared = 48*1024 ;                                                                                                
	                                                                                                                                                       
	cures = cuLaunchKernel (__hybridizer__cufunc, gridDim_x, gridDim_y, 1, blockDim_x, blockDim_y, blockDim_z, shared, 0, __hybridizer_launch_config, 0) ; 
	if (cures != CUDA_SUCCESS) return (int)cures ; 
	int cudaLaunchRes = (int)::cudaPeekAtLastError ();                                                                                                     
	if (cudaLaunchRes != 0) return cudaLaunchRes;                                                                                                          
	int __synchronizeRes = (int)::cudaDeviceSynchronize () ;                                                                                               
	return __synchronizeRes ;                                                                                                                              

}

extern "C" DLL_PUBLIC int Hybridx46Programx46Multiply_pre_basis_ExternCWrapper_CUDA( int gridDim_x,  int gridDim_y,  int blockDim_x,  int blockDim_y,  int blockDim_z,  int shared,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const big_pre_basis,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const small_pre_basis,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const result,  int big_pre_basisLengthAxis1,  int small_pre_basisLengthAxis0,  int small_pre_basisLengthAxis1)
{
	CUresult cures ;                                                                                 
	if (__hybridizer__gs_module.module_data == 0)                                                    
	{                                                                                              
		cures = cuModuleLoadData (&(__hybridizer__gs_module.module), __hybridizer_cubin_module_data) ; 
		if (cures != CUDA_SUCCESS) return (int)cures ;                                                 
	}                                                                                              
	                                                                                                 
	CUfunction __hybridizer__cufunc ;                                                                
	                                                                                                 
	cures = cuModuleGetFunction (&__hybridizer__cufunc, __hybridizer__gs_module.module, "Hybridx46Programx46Multiply_pre_basis") ;   
	if (cures != CUDA_SUCCESS) return (int)cures ;                                                   
	                                                                                                 
	hybridizer::runtime* __hybridizer_runtime = (hybridizer::runtime*) __hybridizer_init_basic_runtime(); 



	void* __hybridizer_launch_config[8] = 
		{
			(void*)&__hybridizer_runtime,
			(void*)&big_pre_basis,
			(void*)&small_pre_basis,
			(void*)&result,
			(void*)&big_pre_basisLengthAxis1,
			(void*)&small_pre_basisLengthAxis0,
			(void*)&small_pre_basisLengthAxis1,
			(void*)0
		} ;

	shared += 16 ; if (shared > 48*1024) shared = 48*1024 ;                                                                                                
	                                                                                                                                                       
	cures = cuLaunchKernel (__hybridizer__cufunc, gridDim_x, gridDim_y, 1, blockDim_x, blockDim_y, blockDim_z, shared, 0, __hybridizer_launch_config, 0) ; 
	if (cures != CUDA_SUCCESS) return (int)cures ; 
	int cudaLaunchRes = (int)::cudaPeekAtLastError ();                                                                                                     
	if (cudaLaunchRes != 0) return cudaLaunchRes;                                                                                                          
	int __synchronizeRes = (int)::cudaDeviceSynchronize () ;                                                                                               
	return __synchronizeRes ;                                                                                                                              

}

extern "C" DLL_PUBLIC int Hybridx46Programx46Simplify_ExternCWrapper_CUDA( int gridDim_x,  int gridDim_y,  int blockDim_x,  int blockDim_y,  int blockDim_z,  int shared,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const ar,  Hybridizer::Runtime::CUDAImports::IntResidentArray* const gcds,  int arLengthAxis0,  int arLengthAxis1)
{
	CUresult cures ;                                                                                 
	if (__hybridizer__gs_module.module_data == 0)                                                    
	{                                                                                              
		cures = cuModuleLoadData (&(__hybridizer__gs_module.module), __hybridizer_cubin_module_data) ; 
		if (cures != CUDA_SUCCESS) return (int)cures ;                                                 
	}                                                                                              
	                                                                                                 
	CUfunction __hybridizer__cufunc ;                                                                
	                                                                                                 
	cures = cuModuleGetFunction (&__hybridizer__cufunc, __hybridizer__gs_module.module, "Hybridx46Programx46Simplify") ;   
	if (cures != CUDA_SUCCESS) return (int)cures ;                                                   
	                                                                                                 
	hybridizer::runtime* __hybridizer_runtime = (hybridizer::runtime*) __hybridizer_init_basic_runtime(); 



	void* __hybridizer_launch_config[6] = 
		{
			(void*)&__hybridizer_runtime,
			(void*)&ar,
			(void*)&gcds,
			(void*)&arLengthAxis0,
			(void*)&arLengthAxis1,
			(void*)0
		} ;

	shared += 16 ; if (shared > 48*1024) shared = 48*1024 ;                                                                                                
	                                                                                                                                                       
	cures = cuLaunchKernel (__hybridizer__cufunc, gridDim_x, gridDim_y, 1, blockDim_x, blockDim_y, blockDim_z, shared, 0, __hybridizer_launch_config, 0) ; 
	if (cures != CUDA_SUCCESS) return (int)cures ; 
	int cudaLaunchRes = (int)::cudaPeekAtLastError ();                                                                                                     
	if (cudaLaunchRes != 0) return cudaLaunchRes;                                                                                                          
	int __synchronizeRes = (int)::cudaDeviceSynchronize () ;                                                                                               
	return __synchronizeRes ;                                                                                                                              

}

#pragma endregion
